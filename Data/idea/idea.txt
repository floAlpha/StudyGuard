[time]2019年02月20日 00:50[time] [content]Estimator API 用于针对分布式环境训练模型。它适用于一些行业使用场景，例如用大型数据集进行分布式训练并导出模型以用于生产。tf.keras.Model 可以通过 tf.estimator API 进行训练，方法是将该模型转换为 tf.estimator.Estimator 对象（通过 tf.keras.estimator.model_to_estimator）。请参阅用 Keras 模型创建 Estimator。model = tf.keras.Sequential([layers.Dense(10,activation='softmax'),                          layers.Dense(10,activation='softmax')])model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),              loss='categorical_crossentropy',              metrics=['accuracy'])estimator = tf.keras.estimator.model_to_estimator(model)[content] [tag]DISPLAY[tag]

[time]2019年02月20日 14:51[time] [content]WordPiece的一种主要的实现方式叫做BPE（Byte-Pair Encoding）双字节编码。 BPE的过程可以理解为把一个单词再拆分，使得我们的此表会变得精简，并且寓意更加清晰。比如"loved","loving","loves"这三个单词。其实本身的语义都是“爱”的意思，但是如果我们以单词为单位，那它们就算不一样的词，在英语中不同后缀的词非常的多，就会使得词表变的很大，训练速度变慢，训练的效果也不是太好。  BPE算法通过训练，能够把上面的3个单词拆分成"lov","ed","ing","es"几部分，这样可以把词的本身的意思和时态分开，有效的减少了词表的数量。WordPiece的一种主要的实现方式叫做BPE（Byte-Pair Encoding）双字节编码。[content] [tag]NODISPLAY[tag]

[time]2019年02月20日 14:53[time] [content]召回率（recall）召回率是覆盖面的度量，度量有多个正例被分为正例，recall=TP/(TP+FN)=TP/P=sensitive，可以看到召回率与灵敏度是一样的。[content] [tag]NODISPLAY[tag]

[time]2019年02月20日 14:53[time] [content]灵敏度（sensitive）sensitive = TP/P，表示的是所有正例中被分对的比例，衡量了分类器对正例的识别能力。 [content] [tag]NODISPLAY[tag]

[time]2019年02月20日 14:53[time] [content]精确率、精度（Precision）表示被分为正例的示例中实际为正例的比例。[content] [tag]NODISPLAY[tag]

[time]2019年02月20日 14:55[time] [content]综合评价指标（F-Measure）P和R指标有时候会出现的矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure（又称为F-Score）。F1综合了P和R的结果，当F1较高时则能说明试验方法比较有效。[content] [tag]DISPLAY[tag]

[time]2019年02月20日 15:25[time] [content]word2vec or GloVe上下文无关[content] [tag]NODISPLAY[tag]

[time]2019年02月20日 15:27[time] [content]ELMo和ULMFit都是单向的或浅双向的。这意味着每个单词只使用其左边(或右边)的单词进行上下文化。[content] [tag]NODISPLAY[tag]

[time]2019年02月20日 16:55[time] [content]Tensor2Tensor notebook[content] [tag]DISPLAY[tag]

[time]2019年02月21日 10:14[time] [content]BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder，因为decoder是不能获要预测的信息的。模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。[content] [tag]DISPLAY[tag]

[time]2019年02月21日 14:27[time] [content]tf.keras 模型可以用tf.contrib.distribute.DistributionStrategy 在多个 GPU 上运行。此 API 在多个 GPU 上提供分布式训练，几乎不需要更改现有代码。目前，tf.contrib.distribute.MirroredStrategy 是唯一受支持的分布策略。MirroredStrategy 通过在一台机器上使用规约在同步训练中进行图内复制。要将 DistributionStrategy 与 Keras 搭配使用，请将 tf.keras.Model 转换为 tf.estimator.Estimator（通过 tf.keras.estimator.model_to_estimator），然后训练该 Estimator以下示例在一台机器上的多个 GPU 间分布了 tf.keras.Model。[content] [tag]NODISPLAY[tag]

[time]2019年02月21日 16:33[time] [content]BERT https://zhuanlan.zhihu.com/p/54356280[content] [tag]NODISPLAY[tag]

[time]2019年02月21日 16:33[time] [content]学习前向传播原理和后向传播[content] [tag]NODISPLAY[tag]

[time]2019年02月21日 20:25[time] [content]Keras model的用法model = keras.Sequential()model.add(keras.layers.Embedding(vocab_size, 16))model.add(keras.layers.GlobalAveragePooling1D())model.add(keras.layers.Dense(16, activation=tf.nn.relu))model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))model.summary()[content] [tag]DISPLAY[tag]

[time]2019年02月22日 11:08[time] [content]tf.keras 模型可以使用 tf.contrib.distribute.DistributionStrategy 在多个 GPU 上运行。此 API 在多个 GPU 上提供分布式训练，几乎不需要更改现有代码。目前，tf.contrib.distribute.MirroredStrategy 是唯一受支持的分布策略。MirroredStrategy 通过在一台机器上使用规约在同步训练中进行图内复制。要将 DistributionStrategy 与 Keras 搭配使用，请将 tf.keras.Model 转换为 tf.estimator.Estimator（通过 tf.keras.estimator.model_to_estimator），然后训练该 Estimator以下示例在一台机器上的多个 GPU 间分布了 tf.keras.Model。[content] [tag]DISPLAY[tag]

